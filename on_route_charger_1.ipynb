{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db212038",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "import json\n",
    "import ast\n",
    "import math\n",
    "from typing import Any, Dict, List, Set, Tuple\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import folium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "811de54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load GTFS files ---\n",
    "GTFS_DIR = \"gtfs_bus_only\" \n",
    "TRIPS_PATH = os.path.join(GTFS_DIR, \"trips.txt\")\n",
    "STOP_TIMES_PATH = os.path.join(GTFS_DIR, \"stop_times.txt\")\n",
    "STOPS_PATH = os.path.join(GTFS_DIR, \"stops.txt\")\n",
    "ROUTES_PATH = os.path.join(GTFS_DIR, \"routes.txt\")\n",
    "trips = pd.read_csv(TRIPS_PATH)\n",
    "stop_times = pd.read_csv(STOP_TIMES_PATH)\n",
    "stops = pd.read_csv(STOPS_PATH)\n",
    "routes = pd.read_csv(ROUTES_PATH)\n",
    "\n",
    "BLOCK_SUMMARY_PATH = \"block_analysis_final.csv\"\n",
    "block_success_summary = pd.read_csv(BLOCK_SUMMARY_PATH)\n",
    "\n",
    "\n",
    "valid_block_ids = set(block_success_summary[\"block_id\"].unique())\n",
    "\n",
    "\n",
    "trips = trips[trips[\"block_id\"].isin(valid_block_ids)]\n",
    "\n",
    "valid_trip_ids = set(trips[\"trip_id\"].unique())\n",
    "stop_times = stop_times[stop_times[\"trip_id\"].isin(valid_trip_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6736cdf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trips_with_route = trips.merge(\n",
    "    routes[[\"route_id\", \"route_short_name\"]],\n",
    "    on=\"route_id\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "\n",
    "trip_to_block = (\n",
    "    trips_with_route\n",
    "    .set_index(\"trip_id\")[\"block_id\"]\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "\n",
    "trip_to_route_short = (\n",
    "    trips_with_route\n",
    "    .set_index(\"trip_id\")[\"route_short_name\"]\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "block_to_asset_class = (\n",
    "    block_success_summary\n",
    "    .set_index(\"block_id\")[\"asset_class\"]\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "block_to_depot = (\n",
    "    block_success_summary\n",
    "    .set_index(\"block_id\")[\"depot_code\"]\n",
    "    .to_dict()\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2c1859",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_times = stop_times.merge(\n",
    "    stops[[\"stop_id\", \"stop_code\"]],\n",
    "    on=\"stop_id\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "stop_times_sorted = stop_times.sort_values([\"trip_id\", \"stop_sequence\"])\n",
    "\n",
    "first_stops = (\n",
    "    stop_times_sorted\n",
    "    .groupby(\"trip_id\", as_index=False)\n",
    "    .first()[[\"trip_id\", \"stop_code\"]]\n",
    ")\n",
    "\n",
    "last_stops = (\n",
    "    stop_times_sorted\n",
    "    .groupby(\"trip_id\", as_index=False)\n",
    "    .last()[[\"trip_id\", \"stop_code\"]]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0859601",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_dict = defaultdict(set)\n",
    "for _, row in first_stops.dropna(subset=[\"stop_code\"]).iterrows():\n",
    "    start_dict[row[\"stop_code\"]].add(row[\"trip_id\"])\n",
    "\n",
    "\n",
    "end_dict = defaultdict(set)\n",
    "for _, row in last_stops.dropna(subset=[\"stop_code\"]).iterrows():\n",
    "    end_dict[row[\"stop_code\"]].add(row[\"trip_id\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bab1016c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stop_codes = sorted(set(start_dict.keys()) | set(end_dict.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac7b941f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for code in all_stop_codes:\n",
    "    start_set = start_dict.get(code, set())\n",
    "    end_set = end_dict.get(code, set())\n",
    "    all_trips = start_set | end_set \n",
    "\n",
    "    rows.append({\n",
    "        \"stop_code\": code,\n",
    "        \"start_trip_ids\": start_set,\n",
    "        \"end_trip_ids\": end_set,\n",
    "        \"all_trip_ids\": all_trips,\n",
    "        \"n_trips\": len(all_trips),\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aebb0ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_sets_df = pd.DataFrame(rows)\n",
    "stops_key = (\n",
    "    stops\n",
    "    .drop_duplicates(subset=[\"stop_code\"])\n",
    "    [[\"stop_code\", \"stop_name\", \"stop_lat\", \"stop_lon\"]]\n",
    ")\n",
    "result_df = trip_sets_df.merge(stops_key, on=\"stop_code\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9b63201b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trips_to_block_set(trip_ids):\n",
    "    \"\"\"\n",
    "    Given an iterable of trip_ids, return a set of block_ids.\n",
    "    \"\"\"\n",
    "    if not isinstance(trip_ids, (list, set, tuple, np.ndarray)):\n",
    "        trip_ids = [trip_ids]\n",
    "\n",
    "    blocks = set()\n",
    "    for t in trip_ids:\n",
    "        b = trip_to_block.get(t)\n",
    "        if pd.notna(b):\n",
    "            blocks.add(b)\n",
    "    return blocks\n",
    "\n",
    "\n",
    "def trips_to_route_short_set(trip_ids):\n",
    "    \"\"\"\n",
    "    Given an iterable of trip_ids, return a set of route_short_name.\n",
    "    \"\"\"\n",
    "    if not isinstance(trip_ids, (list, set, tuple, np.ndarray)):\n",
    "        trip_ids = [trip_ids]\n",
    "\n",
    "    routes_short = set()\n",
    "    for t in trip_ids:\n",
    "        r = trip_to_route_short.get(t)\n",
    "        if isinstance(r, str) and r.strip() != \"\":\n",
    "            routes_short.add(r)\n",
    "    return routes_short\n",
    "\n",
    "\n",
    "def blocks_to_bus_type_set(block_ids):\n",
    "    \"\"\"\n",
    "    Given an iterable of block_ids, return a set of bus types (asset_class).\n",
    "    \"\"\"\n",
    "    if not isinstance(block_ids, (list, set, tuple, np.ndarray)):\n",
    "        block_ids = [block_ids]\n",
    "\n",
    "    types = set()\n",
    "    for b in block_ids:\n",
    "        t = block_to_asset_class.get(b)\n",
    "        if isinstance(t, str) and t.strip() != \"\":\n",
    "            types.add(t)\n",
    "    return types\n",
    "\n",
    "def blocks_to_depot_set(block_ids):\n",
    "    \"\"\"\n",
    "    Given a set/list of block_ids, return the set of depot codes.\n",
    "    \"\"\"\n",
    "    if not isinstance(block_ids, (list, set, tuple, np.ndarray)):\n",
    "        block_ids = [block_ids]\n",
    "\n",
    "    depots = set()\n",
    "    for b in block_ids:\n",
    "        d = block_to_depot.get(b)\n",
    "        if isinstance(d, str) and d.strip() != \"\":\n",
    "            depots.add(d)\n",
    "    return depots\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "89b9cb03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stop_name_simple</th>\n",
       "      <th>stop_code</th>\n",
       "      <th>start_trip_ids</th>\n",
       "      <th>end_trip_ids</th>\n",
       "      <th>all_trip_ids</th>\n",
       "      <th>stop_name</th>\n",
       "      <th>stop_lat</th>\n",
       "      <th>stop_lon</th>\n",
       "      <th>num_trip_total</th>\n",
       "      <th>num_trip_starts</th>\n",
       "      <th>num_trip_ends</th>\n",
       "      <th>block_ids</th>\n",
       "      <th>num_unique_blocks</th>\n",
       "      <th>route_short_names</th>\n",
       "      <th>bus_types</th>\n",
       "      <th>depot_codes</th>\n",
       "      <th>num_unique_depots</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>UBC Exchange</td>\n",
       "      <td>[58895.0, 60158.0, 60159.0, 60160.0, 60162.0, ...</td>\n",
       "      <td>[14828544.0, 14828545.0, 14828546.0, 14828547....</td>\n",
       "      <td>[14828633.0, 14828635.0, 14828636.0, 14828637....</td>\n",
       "      <td>[14828544.0, 14828545.0, 14828546.0, 14828547....</td>\n",
       "      <td>UBC Exchange @ Bay 9</td>\n",
       "      <td>49.265726</td>\n",
       "      <td>-123.248724</td>\n",
       "      <td>4875</td>\n",
       "      <td>2429</td>\n",
       "      <td>2446</td>\n",
       "      <td>[2159391.0, 2159392.0, 2159393.0, 2159394.0, 2...</td>\n",
       "      <td>493</td>\n",
       "      <td>[004, 009, 014, 025, 033, 044, 049, 068, 084, ...</td>\n",
       "      <td>[DS40LF, EL40LF, GS28CS, HY40LF, HY60LF]</td>\n",
       "      <td>[BTC, HTC, RTC, VTC, XHT]</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>Surrey Central Station</td>\n",
       "      <td>[54993.0, 55070.0, 55210.0, 55441.0, 55612.0, ...</td>\n",
       "      <td>[14862491.0, 14862492.0, 14862493.0, 14862494....</td>\n",
       "      <td>[14862463.0, 14862464.0, 14862465.0, 14862466....</td>\n",
       "      <td>[14862463.0, 14862464.0, 14862465.0, 14862466....</td>\n",
       "      <td>Surrey Central Station @ Bay 8</td>\n",
       "      <td>49.188961</td>\n",
       "      <td>-122.848754</td>\n",
       "      <td>4819</td>\n",
       "      <td>2394</td>\n",
       "      <td>2425</td>\n",
       "      <td>[2160043.0, 2160048.0, 2160353.0, 2160513.0, 2...</td>\n",
       "      <td>464</td>\n",
       "      <td>[314, 316, 320, 321, 323, 324, 325, 326, 329, ...</td>\n",
       "      <td>[DS40LF, GS28CS, HY40LF, HY60LF, NG40LF]</td>\n",
       "      <td>[BTC, HTC, STC, XHT]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Coquitlam Central Station</td>\n",
       "      <td>[53021.0, 53035.0, 53161.0, 53170.0, 53299.0, ...</td>\n",
       "      <td>[14849948.0, 14849949.0, 14849950.0, 14849951....</td>\n",
       "      <td>[14849904.0, 14849905.0, 14849906.0, 14849907....</td>\n",
       "      <td>[14849904.0, 14849905.0, 14849906.0, 14849907....</td>\n",
       "      <td>Coquitlam Central Station @ Bay 3</td>\n",
       "      <td>49.275352</td>\n",
       "      <td>-122.798690</td>\n",
       "      <td>4451</td>\n",
       "      <td>2212</td>\n",
       "      <td>2239</td>\n",
       "      <td>[2160407.0, 2160410.0, 2160412.0, 2160420.0, 2...</td>\n",
       "      <td>281</td>\n",
       "      <td>[151, 152, 153, 159, 169, 171, 172, 173, 174, ...</td>\n",
       "      <td>[GS28CS, HY60LF, NG40LF]</td>\n",
       "      <td>[PCT, XNE]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Newton Exchange</td>\n",
       "      <td>[55411.0, 55456.0, 55538.0, 55578.0, 55645.0, ...</td>\n",
       "      <td>[14861807.0, 14861808.0, 14861809.0, 14861810....</td>\n",
       "      <td>[14861757.0, 14861758.0, 14861759.0, 14861760....</td>\n",
       "      <td>[14861757.0, 14861758.0, 14861759.0, 14861760....</td>\n",
       "      <td>Newton Exchange @ Bay 3</td>\n",
       "      <td>49.133297</td>\n",
       "      <td>-122.842298</td>\n",
       "      <td>4298</td>\n",
       "      <td>2156</td>\n",
       "      <td>2142</td>\n",
       "      <td>[2161438.0, 2161439.0, 2161440.0, 2161441.0, 2...</td>\n",
       "      <td>430</td>\n",
       "      <td>[301, 319, 321, 322, 323, 324, 325, 335, 341, ...</td>\n",
       "      <td>[DS40LF, DS44DD, GS28CS, HY40LF, HY60LF, NG40LF]</td>\n",
       "      <td>[HTC, RTC, STC, XHT]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>Metrotown Station</td>\n",
       "      <td>[51164.0, 51999.0, 52390.0, 52499.0, 52646.0, ...</td>\n",
       "      <td>[14838214.0, 14838215.0, 14838216.0, 14838217....</td>\n",
       "      <td>[14838167.0, 14838168.0, 14838169.0, 14838170....</td>\n",
       "      <td>[14838167.0, 14838168.0, 14838169.0, 14838170....</td>\n",
       "      <td>Metrotown Station @ Bay 2</td>\n",
       "      <td>49.226085</td>\n",
       "      <td>-123.002888</td>\n",
       "      <td>4075</td>\n",
       "      <td>2027</td>\n",
       "      <td>2048</td>\n",
       "      <td>[2159680.0, 2159695.0, 2159822.0, 2159828.0, 2...</td>\n",
       "      <td>357</td>\n",
       "      <td>[019, 031, 049, 110, 116, 119, 129, 130, 144, ...</td>\n",
       "      <td>[DS40LF, EL40LF, GS28CS, HY40LF, HY60LF, NG40LF]</td>\n",
       "      <td>[BTC, HTC, PCT, RTC, VTC, XHT]</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>Westbound Mathers Ave</td>\n",
       "      <td>[54698.0]</td>\n",
       "      <td>[15073386.0]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[15073386.0]</td>\n",
       "      <td>Westbound Mathers Ave @ 20 St</td>\n",
       "      <td>49.338261</td>\n",
       "      <td>-123.166908</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[2176194.0]</td>\n",
       "      <td>1</td>\n",
       "      <td>[250]</td>\n",
       "      <td>[DS40LF]</td>\n",
       "      <td>[XWV]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>Westbound 91 Ave</td>\n",
       "      <td>[58515.0]</td>\n",
       "      <td>[14866264.0]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[14866264.0]</td>\n",
       "      <td>Westbound 91 Ave @ 152 St</td>\n",
       "      <td>49.168347</td>\n",
       "      <td>-122.800512</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[2161647.0]</td>\n",
       "      <td>1</td>\n",
       "      <td>[335]</td>\n",
       "      <td>[NG40LF]</td>\n",
       "      <td>[STC]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Eastbound Clements Ave</td>\n",
       "      <td>[54548.0]</td>\n",
       "      <td>[14857730.0]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[14857730.0]</td>\n",
       "      <td>Eastbound Clements Ave @ Capilano Rd</td>\n",
       "      <td>49.357970</td>\n",
       "      <td>-123.106332</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[2165067.0]</td>\n",
       "      <td>1</td>\n",
       "      <td>[232]</td>\n",
       "      <td>[DS40LF]</td>\n",
       "      <td>[BTC]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>Westbound Cambie Rd</td>\n",
       "      <td>[56927.0]</td>\n",
       "      <td>[14872712.0]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[14872712.0]</td>\n",
       "      <td>Westbound Cambie Rd @ Jacombs Rd</td>\n",
       "      <td>49.184628</td>\n",
       "      <td>-123.080850</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[2162314.0]</td>\n",
       "      <td>1</td>\n",
       "      <td>[407]</td>\n",
       "      <td>[DS40LF]</td>\n",
       "      <td>[RTC]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>Westbound E Broadway</td>\n",
       "      <td>[50907.0]</td>\n",
       "      <td>[14901894.0]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[14901894.0]</td>\n",
       "      <td>Westbound E Broadway @ Renfrew St</td>\n",
       "      <td>49.262097</td>\n",
       "      <td>-123.044962</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[2164545.0]</td>\n",
       "      <td>1</td>\n",
       "      <td>[009]</td>\n",
       "      <td>[HY40LF]</td>\n",
       "      <td>[VTC]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              stop_name_simple  \\\n",
       "230               UBC Exchange   \n",
       "228     Surrey Central Station   \n",
       "18   Coquitlam Central Station   \n",
       "99             Newton Exchange   \n",
       "93           Metrotown Station   \n",
       "..                         ...   \n",
       "275      Westbound Mathers Ave   \n",
       "247           Westbound 91 Ave   \n",
       "33      Eastbound Clements Ave   \n",
       "254        Westbound Cambie Rd   \n",
       "263       Westbound E Broadway   \n",
       "\n",
       "                                             stop_code  \\\n",
       "230  [58895.0, 60158.0, 60159.0, 60160.0, 60162.0, ...   \n",
       "228  [54993.0, 55070.0, 55210.0, 55441.0, 55612.0, ...   \n",
       "18   [53021.0, 53035.0, 53161.0, 53170.0, 53299.0, ...   \n",
       "99   [55411.0, 55456.0, 55538.0, 55578.0, 55645.0, ...   \n",
       "93   [51164.0, 51999.0, 52390.0, 52499.0, 52646.0, ...   \n",
       "..                                                 ...   \n",
       "275                                          [54698.0]   \n",
       "247                                          [58515.0]   \n",
       "33                                           [54548.0]   \n",
       "254                                          [56927.0]   \n",
       "263                                          [50907.0]   \n",
       "\n",
       "                                        start_trip_ids  \\\n",
       "230  [14828544.0, 14828545.0, 14828546.0, 14828547....   \n",
       "228  [14862491.0, 14862492.0, 14862493.0, 14862494....   \n",
       "18   [14849948.0, 14849949.0, 14849950.0, 14849951....   \n",
       "99   [14861807.0, 14861808.0, 14861809.0, 14861810....   \n",
       "93   [14838214.0, 14838215.0, 14838216.0, 14838217....   \n",
       "..                                                 ...   \n",
       "275                                       [15073386.0]   \n",
       "247                                       [14866264.0]   \n",
       "33                                        [14857730.0]   \n",
       "254                                       [14872712.0]   \n",
       "263                                       [14901894.0]   \n",
       "\n",
       "                                          end_trip_ids  \\\n",
       "230  [14828633.0, 14828635.0, 14828636.0, 14828637....   \n",
       "228  [14862463.0, 14862464.0, 14862465.0, 14862466....   \n",
       "18   [14849904.0, 14849905.0, 14849906.0, 14849907....   \n",
       "99   [14861757.0, 14861758.0, 14861759.0, 14861760....   \n",
       "93   [14838167.0, 14838168.0, 14838169.0, 14838170....   \n",
       "..                                                 ...   \n",
       "275                                                 []   \n",
       "247                                                 []   \n",
       "33                                                  []   \n",
       "254                                                 []   \n",
       "263                                                 []   \n",
       "\n",
       "                                          all_trip_ids  \\\n",
       "230  [14828544.0, 14828545.0, 14828546.0, 14828547....   \n",
       "228  [14862463.0, 14862464.0, 14862465.0, 14862466....   \n",
       "18   [14849904.0, 14849905.0, 14849906.0, 14849907....   \n",
       "99   [14861757.0, 14861758.0, 14861759.0, 14861760....   \n",
       "93   [14838167.0, 14838168.0, 14838169.0, 14838170....   \n",
       "..                                                 ...   \n",
       "275                                       [15073386.0]   \n",
       "247                                       [14866264.0]   \n",
       "33                                        [14857730.0]   \n",
       "254                                       [14872712.0]   \n",
       "263                                       [14901894.0]   \n",
       "\n",
       "                                stop_name   stop_lat    stop_lon  \\\n",
       "230                  UBC Exchange @ Bay 9  49.265726 -123.248724   \n",
       "228        Surrey Central Station @ Bay 8  49.188961 -122.848754   \n",
       "18      Coquitlam Central Station @ Bay 3  49.275352 -122.798690   \n",
       "99                Newton Exchange @ Bay 3  49.133297 -122.842298   \n",
       "93              Metrotown Station @ Bay 2  49.226085 -123.002888   \n",
       "..                                    ...        ...         ...   \n",
       "275         Westbound Mathers Ave @ 20 St  49.338261 -123.166908   \n",
       "247             Westbound 91 Ave @ 152 St  49.168347 -122.800512   \n",
       "33   Eastbound Clements Ave @ Capilano Rd  49.357970 -123.106332   \n",
       "254      Westbound Cambie Rd @ Jacombs Rd  49.184628 -123.080850   \n",
       "263     Westbound E Broadway @ Renfrew St  49.262097 -123.044962   \n",
       "\n",
       "     num_trip_total  num_trip_starts  num_trip_ends  \\\n",
       "230            4875             2429           2446   \n",
       "228            4819             2394           2425   \n",
       "18             4451             2212           2239   \n",
       "99             4298             2156           2142   \n",
       "93             4075             2027           2048   \n",
       "..              ...              ...            ...   \n",
       "275               1                1              0   \n",
       "247               1                1              0   \n",
       "33                1                1              0   \n",
       "254               1                1              0   \n",
       "263               1                1              0   \n",
       "\n",
       "                                             block_ids  num_unique_blocks  \\\n",
       "230  [2159391.0, 2159392.0, 2159393.0, 2159394.0, 2...                493   \n",
       "228  [2160043.0, 2160048.0, 2160353.0, 2160513.0, 2...                464   \n",
       "18   [2160407.0, 2160410.0, 2160412.0, 2160420.0, 2...                281   \n",
       "99   [2161438.0, 2161439.0, 2161440.0, 2161441.0, 2...                430   \n",
       "93   [2159680.0, 2159695.0, 2159822.0, 2159828.0, 2...                357   \n",
       "..                                                 ...                ...   \n",
       "275                                        [2176194.0]                  1   \n",
       "247                                        [2161647.0]                  1   \n",
       "33                                         [2165067.0]                  1   \n",
       "254                                        [2162314.0]                  1   \n",
       "263                                        [2164545.0]                  1   \n",
       "\n",
       "                                     route_short_names  \\\n",
       "230  [004, 009, 014, 025, 033, 044, 049, 068, 084, ...   \n",
       "228  [314, 316, 320, 321, 323, 324, 325, 326, 329, ...   \n",
       "18   [151, 152, 153, 159, 169, 171, 172, 173, 174, ...   \n",
       "99   [301, 319, 321, 322, 323, 324, 325, 335, 341, ...   \n",
       "93   [019, 031, 049, 110, 116, 119, 129, 130, 144, ...   \n",
       "..                                                 ...   \n",
       "275                                              [250]   \n",
       "247                                              [335]   \n",
       "33                                               [232]   \n",
       "254                                              [407]   \n",
       "263                                              [009]   \n",
       "\n",
       "                                            bus_types  \\\n",
       "230          [DS40LF, EL40LF, GS28CS, HY40LF, HY60LF]   \n",
       "228          [DS40LF, GS28CS, HY40LF, HY60LF, NG40LF]   \n",
       "18                           [GS28CS, HY60LF, NG40LF]   \n",
       "99   [DS40LF, DS44DD, GS28CS, HY40LF, HY60LF, NG40LF]   \n",
       "93   [DS40LF, EL40LF, GS28CS, HY40LF, HY60LF, NG40LF]   \n",
       "..                                                ...   \n",
       "275                                          [DS40LF]   \n",
       "247                                          [NG40LF]   \n",
       "33                                           [DS40LF]   \n",
       "254                                          [DS40LF]   \n",
       "263                                          [HY40LF]   \n",
       "\n",
       "                        depot_codes  num_unique_depots  \n",
       "230       [BTC, HTC, RTC, VTC, XHT]                  5  \n",
       "228            [BTC, HTC, STC, XHT]                  4  \n",
       "18                       [PCT, XNE]                  2  \n",
       "99             [HTC, RTC, STC, XHT]                  4  \n",
       "93   [BTC, HTC, PCT, RTC, VTC, XHT]                  6  \n",
       "..                              ...                ...  \n",
       "275                           [XWV]                  1  \n",
       "247                           [STC]                  1  \n",
       "33                            [BTC]                  1  \n",
       "254                           [RTC]                  1  \n",
       "263                           [VTC]                  1  \n",
       "\n",
       "[300 rows x 17 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df[\"stop_name_simple\"] = (\n",
    "    result_df[\"stop_name\"]\n",
    "    .astype(str)\n",
    "    .str.replace(r\"\\s*@.*$\", \"\", regex=True)\n",
    "    .str.strip()\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "def union_sets(series):\n",
    "    combined = set()\n",
    "    for x in series:\n",
    "        if pd.isna(x):\n",
    "            continue\n",
    "        combined |= set(x if isinstance(x, (set, list)) else [x])\n",
    "    return combined\n",
    "\n",
    "grouped_df = (\n",
    "    result_df\n",
    "    .groupby(\"stop_name_simple\", as_index=False)\n",
    "    .agg({\n",
    "        \"stop_code\":      union_sets,\n",
    "        \"start_trip_ids\": union_sets,\n",
    "        \"end_trip_ids\":   union_sets,\n",
    "        \"all_trip_ids\":   union_sets,\n",
    "        \"stop_name\":      \"first\",\n",
    "        \"stop_lat\":       \"first\",\n",
    "        \"stop_lon\":       \"first\",\n",
    "    })\n",
    ")\n",
    "\n",
    "\n",
    "grouped_df[\"num_trip_total\"] = grouped_df[\"all_trip_ids\"].apply(len)\n",
    "grouped_df[\"num_trip_starts\"] = grouped_df[\"start_trip_ids\"].apply(len)\n",
    "grouped_df[\"num_trip_ends\"] = grouped_df[\"end_trip_ids\"].apply(len)\n",
    "grouped_df[\"stop_code\"]      = grouped_df[\"stop_code\"].apply(sorted)\n",
    "grouped_df[\"start_trip_ids\"] = grouped_df[\"start_trip_ids\"].apply(sorted)\n",
    "grouped_df[\"end_trip_ids\"]   = grouped_df[\"end_trip_ids\"].apply(sorted)\n",
    "grouped_df[\"all_trip_ids\"]   = grouped_df[\"all_trip_ids\"].apply(sorted)\n",
    "grouped_df[\"block_ids\"] = grouped_df[\"all_trip_ids\"].apply(trips_to_block_set)\n",
    "grouped_df[\"num_unique_blocks\"] = grouped_df[\"block_ids\"].apply(len)\n",
    "grouped_df[\"route_short_names\"] = grouped_df[\"all_trip_ids\"].apply(trips_to_route_short_set)\n",
    "grouped_df[\"bus_types\"] = grouped_df[\"block_ids\"].apply(blocks_to_bus_type_set)\n",
    "\n",
    "grouped_df[\"block_ids\"]         = grouped_df[\"block_ids\"].apply(lambda s: sorted(s))\n",
    "grouped_df[\"route_short_names\"] = grouped_df[\"route_short_names\"].apply(lambda s: sorted(s))\n",
    "grouped_df[\"bus_types\"]         = grouped_df[\"bus_types\"].apply(lambda s: sorted(s))\n",
    "grouped_df[\"depot_codes\"] = grouped_df[\"block_ids\"].apply(blocks_to_depot_set)\n",
    "grouped_df[\"depot_codes\"] = grouped_df[\"depot_codes\"].apply(lambda s: sorted(s))\n",
    "grouped_df[\"num_unique_depots\"] = grouped_df[\"depot_codes\"].apply(len)\n",
    "grouped_df.sort_values('num_trip_total', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a50d89cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Columns that contain sets/lists\n",
    "complex_cols = [\n",
    "    \"stop_code\",\n",
    "    \"start_trip_ids\",\n",
    "    \"end_trip_ids\",\n",
    "    \"all_trip_ids\",\n",
    "    \"block_ids\",\n",
    "    \"route_short_names\",\n",
    "    \"bus_types\",\n",
    "    \"depot_codes\"\n",
    "]\n",
    "\n",
    "df_to_save = grouped_df.copy()\n",
    "\n",
    "for col in complex_cols:\n",
    "    df_to_save[col] = df_to_save[col].apply(lambda x: json.dumps(list(x)) if isinstance(x, (set, list)) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eae83c11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Commercial-Broadway Station', 'Knight Street-Marine Drive'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "curr_candidate = pd.read_csv(\"on_route_charger_location.txt\")\n",
    "set(curr_candidate['Location']) - set(grouped_df['stop_name_simple'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "096830ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_candidate['Location_new'] = curr_candidate['Location'].replace({\n",
    "    'Commercial-Broadway Station': 'N Grandview Hwy',\n",
    "    'Knight Street-Marine Drive': 'Northbound Knight St Bridge Offramp'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b798dc5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_save_candidate = df_to_save[df_to_save['stop_name_simple'].isin(set(curr_candidate['Location_new']))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "683e9132",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_save_candidate.to_excel('df_to_save_candidate.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e4810b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "locations = set(map(str, curr_candidate['Location_new']))\n",
    "\n",
    "rows = []\n",
    "for _, row in grouped_df.iterrows():\n",
    "    name = str(row['stop_name_simple'])\n",
    "    matching_locs = [loc for loc in locations if name in loc]\n",
    "    if not matching_locs:\n",
    "        continue\n",
    "\n",
    "    codes = row['stop_code']\n",
    "    if not isinstance(codes, (list, set, tuple)):\n",
    "        codes = [codes]\n",
    "\n",
    "    for loc in matching_locs:\n",
    "        for c in codes:\n",
    "            rows.append({\n",
    "                \"candidate_name\": loc,\n",
    "                \"stop_name_simple\": name,\n",
    "                \"stop_code\": int(c),\n",
    "                \"stop_lat\": row[\"stop_lat\"],\n",
    "                \"stop_lon\": row[\"stop_lon\"],\n",
    "                \"route_short_names\": row[\"route_short_names\"],\n",
    "                \"bus_types\": row[\"bus_types\"],\n",
    "                \"num_unique_blocks\": row[\"num_unique_blocks\"],\n",
    "            })\n",
    "\n",
    "candidate_stop_map = pd.DataFrame(rows)\n",
    "\n",
    "matched_codes = set(candidate_stop_map[\"stop_code\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "78d774b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Map saved to terminal_stations_grouped_map.html\n"
     ]
    }
   ],
   "source": [
    "import folium\n",
    "\n",
    "# Center of map\n",
    "center_lat = grouped_df[\"stop_lat\"].mean()\n",
    "center_lon = grouped_df[\"stop_lon\"].mean()\n",
    "\n",
    "m = folium.Map(\n",
    "    location=[center_lat, center_lon],\n",
    "    zoom_start=11,\n",
    "    tiles=\"CartoDB positron\"\n",
    ")\n",
    "\n",
    "max_activity = grouped_df[\"num_trip_total\"].max()\n",
    "\n",
    "# Convert curr_candidate['Location_new'] to a fast lookup set\n",
    "highlight_set = set(curr_candidate['Location_new'])\n",
    "\n",
    "for _, row in grouped_df.iterrows():\n",
    "    activity = row[\"num_trip_total\"]\n",
    "\n",
    "    # Scale radius\n",
    "    radius = 5 + 25 * (activity / max_activity) if max_activity > 0 else 5\n",
    "\n",
    "    # Pretty-print stop codes\n",
    "    stop_codes_str = \", \".join(sorted(map(str, map(int, row[\"stop_code\"]))))\n",
    "\n",
    "    # Pretty-print route_short_names\n",
    "    route_short_names = row.get(\"route_short_names\", [])\n",
    "    if isinstance(route_short_names, (set, list, tuple)):\n",
    "        routes_str = \", \".join(map(str, sorted(route_short_names)))\n",
    "    else:\n",
    "        routes_str = str(route_short_names)\n",
    "\n",
    "    # Pretty-print bus types\n",
    "    bus_types = row.get(\"bus_types\", [])\n",
    "    if isinstance(bus_types, (set, list, tuple)):\n",
    "        bus_types_str = \", \".join(map(str, sorted(bus_types)))\n",
    "    else:\n",
    "        bus_types_str = str(bus_types)\n",
    "\n",
    "    # ✅ Pretty-print depot codes\n",
    "    depot_codes = row.get(\"depot_codes\", [])\n",
    "    if isinstance(depot_codes, (set, list, tuple)):\n",
    "        depot_codes_str = \", \".join(map(str, sorted(depot_codes)))\n",
    "    else:\n",
    "        depot_codes_str = str(depot_codes)\n",
    "\n",
    "    # Marker color logic\n",
    "    if row[\"stop_name_simple\"] in highlight_set:\n",
    "        color = \"green\"\n",
    "        fill_color = \"green\"\n",
    "    else:\n",
    "        color = \"blue\"\n",
    "        fill_color = \"blue\"\n",
    "\n",
    "    folium.CircleMarker(\n",
    "        location=[row[\"stop_lat\"], row[\"stop_lon\"]],\n",
    "        radius=radius,\n",
    "        color=color,\n",
    "        fill=True,\n",
    "        fill_color=fill_color,\n",
    "        fill_opacity=0.4,\n",
    "        weight=1,\n",
    "        popup=folium.Popup(\n",
    "            f\"<b>{row['stop_name_simple']}</b><br>\"\n",
    "            f\"Total Trips: {row['num_trip_total']}<br>\"\n",
    "            f\"Trip Starts: {row['num_trip_starts']}<br>\"\n",
    "            f\"Trip Ends: {row['num_trip_ends']}<br>\"\n",
    "            f\"Stop Codes: {stop_codes_str}<br>\"\n",
    "            f\"Unique Blocks: {row['num_unique_blocks']}<br>\"\n",
    "            f\"Routes: {routes_str}<br>\"\n",
    "            f\"Bus Types: {bus_types_str}<br>\"\n",
    "            f\"Impact Depot: {depot_codes_str}<br>\"                 # NEW LINE\n",
    "            f\"Number of Depots: {row['num_unique_depots']}\",       # NEW LINE\n",
    "            max_width=350\n",
    "        )\n",
    "    ).add_to(m)\n",
    "\n",
    "# Save map\n",
    "m.save(\"terminal_stations_grouped_map.html\")\n",
    "print(\"Map saved to terminal_stations_grouped_map.html\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b5fbc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60beadd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "59370ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Terminal minute occupancy (dropoff / recovery / pickup) from GTFS blocks,\n",
    "using stops.stop_code (NOT stop_id) to identify candidate terminal stops.\n",
    "\n",
    "Key fix implemented:\n",
    "✅ Dominant-duration minute assignment:\n",
    "   For each (terminal, minute, block_id) the bus is assigned to exactly ONE status\n",
    "   (dropoff OR recovery OR pickup) based on the most seconds overlapped in that minute.\n",
    "   This removes the \"duplication\" you observed at minute boundaries.\n",
    "\n",
    "Other requirements implemented:\n",
    "- Run separately for service_id in {1,2,3} (1=MF,2=Sat,3=Sun) and save separate files\n",
    "- No timestamp column; add day_offset and is_next_day (e.g., 25:10 is next day)\n",
    "- Drop rows where total == 0\n",
    "- Add description_json ONLY to terminal_minute_counts_by_terminal_* table\n",
    "  as a JSON string grouped by status and metadata counts.\n",
    "- Metadata (line_group, block_number, asset_class, depot_code) comes from block_analysis_final.csv\n",
    "  joined via normalized block_id (handles int vs '21003.0' etc.)\n",
    "- prev_route_short_name / next_route_short_name comes from GTFS routes.txt (via trips.route_id)\n",
    "\n",
    "Inputs:\n",
    "- GTFS folder with stops.txt, stop_times.txt, trips.txt, routes.txt\n",
    "- Candidate terminal excel: df_to_save_candidate.xlsx (stop_name_simple + stop_code list)\n",
    "- Block meta csv: block_analysis_final.csv\n",
    "\n",
    "Outputs per service_id:\n",
    "- terminal_minute_counts_{MF|Sat|Sun}_service_id_{sid}.csv\n",
    "- terminal_minute_counts_by_terminal_{MF|Sat|Sun}_service_id_{sid}.csv   (includes description_json)\n",
    "- terminal_intervals_{MF|Sat|Sun}_service_id_{sid}.csv\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "import json\n",
    "from collections import defaultdict, Counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# =======================\n",
    "# User inputs / paths\n",
    "# =======================\n",
    "CANDIDATE_XLSX = \"df_to_save_candidate.xlsx\"\n",
    "BLOCK_META_CSV = \"block_analysis_final.csv\"  \n",
    "SERVICE_MAP = {1: \"MF\", 2: \"Sat\", 3: \"Sun\"}\n",
    "\n",
    "\n",
    "# =======================\n",
    "# Parameters (assumptions)\n",
    "# =======================\n",
    "DROP_SEC = 90\n",
    "PICK_SEC = 90\n",
    "DEADHEAD_DIST_M = 500\n",
    "ARRIVE_BEFORE_DEP_SEC = int(6.5 * 60)  # 390\n",
    "RECOVERY_SEC = 5 * 60                  # 300\n",
    "\n",
    "\n",
    "# =======================\n",
    "# Helpers\n",
    "# =======================\n",
    "def parse_gtfs_time_to_sec(t: str) -> int:\n",
    "    \"\"\"GTFS time can exceed 24:00:00 (e.g., 25:13:00). Convert to seconds since service day start.\"\"\"\n",
    "    if pd.isna(t):\n",
    "        return np.nan\n",
    "    hh, mm, ss = t.split(\":\")\n",
    "    return int(hh) * 3600 + int(mm) * 60 + int(ss)\n",
    "\n",
    "\n",
    "def sec_to_hhmm(sec: int) -> str:\n",
    "    \"\"\"Preserves 25:xx style display (no modulo 24h).\"\"\"\n",
    "    hh = sec // 3600\n",
    "    mm = (sec % 3600) // 60\n",
    "    return f\"{hh:02d}:{mm:02d}\"\n",
    "\n",
    "\n",
    "def haversine_m(lat1, lon1, lat2, lon2) -> float:\n",
    "    \"\"\"Great-circle distance in meters.\"\"\"\n",
    "    R = 6371000.0\n",
    "    phi1 = np.radians(lat1)\n",
    "    phi2 = np.radians(lat2)\n",
    "    dphi = np.radians(lat2 - lat1)\n",
    "    dlmb = np.radians(lon2 - lon1)\n",
    "    a = np.sin(dphi / 2.0) ** 2 + np.cos(phi1) * np.cos(phi2) * np.sin(dlmb / 2.0) ** 2\n",
    "    return float(2 * R * np.arctan2(np.sqrt(a), np.sqrt(1 - a)))\n",
    "\n",
    "\n",
    "def _norm_stop_code(x) -> str:\n",
    "    \"\"\"Normalize stop_code coming from Excel and GTFS to a comparable string.\"\"\"\n",
    "    if pd.isna(x):\n",
    "        return \"\"\n",
    "    if isinstance(x, (int, np.integer)):\n",
    "        return str(int(x))\n",
    "    if isinstance(x, (float, np.floating)):\n",
    "        return str(int(x)) if float(x).is_integer() else str(x)\n",
    "    s = str(x).strip()\n",
    "    try:\n",
    "        f = float(s)\n",
    "        if f.is_integer():\n",
    "            return str(int(f))\n",
    "    except Exception:\n",
    "        pass\n",
    "    return s\n",
    "\n",
    "\n",
    "def norm_block_id(x):\n",
    "    \"\"\"Normalize block_id to comparable string: handles int, float-like '21003.0', whitespace.\"\"\"\n",
    "    if pd.isna(x):\n",
    "        return None\n",
    "    s = str(x).strip()\n",
    "    if re.fullmatch(r\"\\d+\\.0\", s):\n",
    "        return s[:-2]\n",
    "    try:\n",
    "        f = float(s)\n",
    "        if f.is_integer():\n",
    "            return str(int(f))\n",
    "    except Exception:\n",
    "        pass\n",
    "    return s\n",
    "\n",
    "\n",
    "def safe_split_same_terminal(arr_s: int, dep_s: int):\n",
    "    \"\"\"\n",
    "    Split [arr_s, dep_s) into dropoff / recovery / pickup robustly even if dwell is short.\n",
    "    Returns list of (status, start, end).\n",
    "    \"\"\"\n",
    "    if dep_s <= arr_s:\n",
    "        return []\n",
    "\n",
    "    dwell = dep_s - arr_s\n",
    "    drop = min(DROP_SEC, dwell / 2.0)\n",
    "    pick = min(PICK_SEC, max(0.0, dwell - drop))\n",
    "    drop = int(math.floor(drop))\n",
    "    pick = int(math.floor(pick))\n",
    "\n",
    "    rec_start = arr_s + drop\n",
    "    rec_end = dep_s - pick\n",
    "\n",
    "    out = []\n",
    "    if drop > 0:\n",
    "        out.append((\"dropoff\", arr_s, arr_s + drop))\n",
    "    if rec_end > rec_start:\n",
    "        out.append((\"recovery\", rec_start, rec_end))\n",
    "    if pick > 0:\n",
    "        out.append((\"pickup\", dep_s - pick, dep_s))\n",
    "    return out\n",
    "\n",
    "\n",
    "def add_interval_minute_diff(diff: np.ndarray, start_s: int, end_s: int):\n",
    "    \"\"\"\n",
    "    Adds +1 to each minute overlapped by [start_s, end_s).\n",
    "    Uses a difference array for O(1) range updates.\n",
    "    \"\"\"\n",
    "    if end_s <= start_s:\n",
    "        return\n",
    "    m0 = start_s // 60\n",
    "    m1 = (end_s - 1) // 60  # inclusive last minute touched\n",
    "    if m0 < 0:\n",
    "        m0 = 0\n",
    "    if m1 < 0:\n",
    "        return\n",
    "    if m0 >= len(diff):\n",
    "        return\n",
    "    m1 = min(m1, len(diff) - 2)  # keep room for m1+1\n",
    "    diff[m0] += 1\n",
    "    diff[m1 + 1] -= 1\n",
    "\n",
    "\n",
    "# def _detect_col(df: pd.DataFrame, candidates: list[str], required: bool = True) -> str:\n",
    "#     \"\"\"Pick the first existing column name from candidates.\"\"\"\n",
    "#     for c in candidates:\n",
    "#         if c in df.columns:\n",
    "#             return c\n",
    "#     if required:\n",
    "#         raise ValueError(f\"Missing required column. Tried: {candidates}. Available: {list(df.columns)[:50]} ...\")\n",
    "#     return \"\"\n",
    "\n",
    "\n",
    "# =======================\n",
    "# Dominant-duration minute assignment\n",
    "# =======================\n",
    "STATUS_PRIORITY = {\"pickup\": 3, \"dropoff\": 2, \"recovery\": 1}  # tie-break only\n",
    "\n",
    "def build_minute_assignment(intervals_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Convert second-level intervals to minute-level assignment:\n",
    "      (terminal, minute_index, block_id) -> exactly ONE status\n",
    "    using dominant overlap seconds in that minute.\n",
    "    \"\"\"\n",
    "    if intervals_df.empty:\n",
    "        return pd.DataFrame(columns=[\"terminal\", \"minute_index\", \"time_sec\", \"block_id\", \"status\", \"dominant_sec\"])\n",
    "\n",
    "    # (terminal, minute, block_id, status) -> overlap_seconds\n",
    "    acc = defaultdict(int)\n",
    "\n",
    "    for _, r in intervals_df.iterrows():\n",
    "        term = str(r[\"terminal\"])\n",
    "        block_id = str(r[\"block_id\"])\n",
    "        status = r[\"status\"]\n",
    "        s = int(r[\"start_sec\"])\n",
    "        e = int(r[\"end_sec\"])\n",
    "        if e <= s:\n",
    "            continue\n",
    "\n",
    "        m0 = s // 60\n",
    "        m1 = (e - 1) // 60\n",
    "\n",
    "        for m in range(m0, m1 + 1):\n",
    "            ms = m * 60\n",
    "            me = (m + 1) * 60\n",
    "            overlap = min(e, me) - max(s, ms)\n",
    "            if overlap > 0:\n",
    "                acc[(term, m, block_id, status)] += overlap\n",
    "\n",
    "    # choose dominant status per (terminal, minute, block_id)\n",
    "    by_key = defaultdict(list)\n",
    "    for (term, m, block_id, status), sec in acc.items():\n",
    "        by_key[(term, m, block_id)].append((status, sec))\n",
    "\n",
    "    out = []\n",
    "    for (term, m, block_id), items in by_key.items():\n",
    "        items_sorted = sorted(\n",
    "            items,\n",
    "            key=lambda x: (x[1], STATUS_PRIORITY.get(x[0], 0)),\n",
    "            reverse=True,\n",
    "        )\n",
    "        best_status, best_sec = items_sorted[0]\n",
    "        out.append({\n",
    "            \"terminal\": term,\n",
    "            \"minute_index\": int(m),\n",
    "            \"time_sec\": int(m) * 60,\n",
    "            \"block_id\": block_id,\n",
    "            \"status\": best_status,\n",
    "            \"dominant_sec\": int(best_sec),\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(out)\n",
    "\n",
    "\n",
    "def build_per_terminal_minute_from_assignment(assign_df: pd.DataFrame, service_id: int, service_label: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Aggregate minute assignment into terminal_minute_counts_by_terminal table.\n",
    "    Ensures no duplication across statuses per bus per minute.\n",
    "    \"\"\"\n",
    "    if assign_df.empty:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    a = assign_df.copy()\n",
    "    a[\"day_offset\"] = (a[\"time_sec\"] // 86400).astype(int)\n",
    "    a[\"is_next_day\"] = (a[\"day_offset\"] >= 1).astype(int)\n",
    "    a[\"time_hhmm\"] = a[\"time_sec\"].apply(sec_to_hhmm)\n",
    "\n",
    "    # each row is (terminal, minute, block_id) with a single status\n",
    "    g = a.groupby([\"terminal\", \"time_sec\", \"time_hhmm\", \"day_offset\", \"is_next_day\"], as_index=False)\n",
    "\n",
    "    rows = []\n",
    "    for _, df in g:\n",
    "        term = df[\"terminal\"].iloc[0]\n",
    "        time_sec = int(df[\"time_sec\"].iloc[0])\n",
    "        time_hhmm = df[\"time_hhmm\"].iloc[0]\n",
    "        day_offset = int(df[\"day_offset\"].iloc[0])\n",
    "        is_next_day = int(df[\"is_next_day\"].iloc[0])\n",
    "\n",
    "        total = int(df[\"block_id\"].nunique())\n",
    "        dropoff = int((df[\"status\"] == \"dropoff\").sum())\n",
    "        recovery = int((df[\"status\"] == \"recovery\").sum())\n",
    "        pickup = int((df[\"status\"] == \"pickup\").sum())\n",
    "\n",
    "        rows.append({\n",
    "            \"service_id\": service_id,\n",
    "            \"service_label\": service_label,\n",
    "            \"terminal\": term,\n",
    "            \"time_sec\": time_sec,\n",
    "            \"time_hhmm\": time_hhmm,\n",
    "            \"day_offset\": day_offset,\n",
    "            \"is_next_day\": is_next_day,\n",
    "            \"total\": total,\n",
    "            \"dropoff\": dropoff,\n",
    "            \"recovery\": recovery,\n",
    "            \"pickup\": pickup,\n",
    "        })\n",
    "\n",
    "    per_terminal_df = pd.DataFrame(rows)\n",
    "    per_terminal_df = per_terminal_df[per_terminal_df[\"total\"] > 0].reset_index(drop=True)\n",
    "    return per_terminal_df\n",
    "\n",
    "\n",
    "def add_description_json_to_per_terminal_from_assignment(\n",
    "    per_terminal_df: pd.DataFrame,\n",
    "    assign_df: pd.DataFrame,\n",
    "    bus_lookup_df: pd.DataFrame,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build description_json for per_terminal_df using the minute assignment table,\n",
    "    so each bus contributes to exactly one status per minute.\n",
    "\n",
    "    JSON groups by status, then by metadata tuple with counts n:\n",
    "      line_group, block_number, asset_class, depot_code, prev_route, next_route\n",
    "    \"\"\"\n",
    "    if per_terminal_df.empty:\n",
    "        out = per_terminal_df.copy()\n",
    "        out[\"description_json\"] = []\n",
    "        return out\n",
    "\n",
    "    # Only keep keys that exist in per_terminal_df\n",
    "    valid_keys = set(zip(per_terminal_df[\"terminal\"].astype(str),\n",
    "                         (per_terminal_df[\"time_sec\"].values // 60).astype(int)))\n",
    "\n",
    "    # Join assignment -> per-boundary metadata lookup\n",
    "    a = assign_df.merge(bus_lookup_df, on=[\"terminal\", \"block_id\", \"time_sec\"], how=\"left\")\n",
    "\n",
    "    key_status_counter = defaultdict(lambda: defaultdict(Counter))\n",
    "\n",
    "    for _, r in a.iterrows():\n",
    "        term = str(r[\"terminal\"])\n",
    "        m = int(r[\"time_sec\"]) // 60\n",
    "        if (term, m) not in valid_keys:\n",
    "            continue\n",
    "\n",
    "        st = r[\"status\"]\n",
    "        meta_key = (\n",
    "            str(r.get(\"line_group\", \"\")),\n",
    "            str(r.get(\"block_number\", \"\")),\n",
    "            str(r.get(\"asset_class\", \"\")),\n",
    "            str(r.get(\"depot_code\", \"\")),\n",
    "            str(r.get(\"prev_route_short_name\", \"\")),\n",
    "            str(r.get(\"next_route_short_name\", \"\")),\n",
    "        )\n",
    "        key_status_counter[(term, m)][st][meta_key] += 1\n",
    "\n",
    "    def norm(x: str):\n",
    "        return None if x in (\"nan\", \"None\", \"\") else x\n",
    "\n",
    "    def key_to_dict(k, n):\n",
    "        return {\n",
    "            \"line_group\": norm(k[0]),\n",
    "            \"block_number\": norm(k[1]),\n",
    "            \"asset_class\": norm(k[2]),\n",
    "            \"depot_code\": norm(k[3]),\n",
    "            \"prev_route\": norm(k[4]),\n",
    "            \"next_route\": norm(k[5]),\n",
    "            \"n\": int(n),\n",
    "        }\n",
    "\n",
    "    json_list = []\n",
    "    for _, row in per_terminal_df.iterrows():\n",
    "        term = str(row[\"terminal\"])\n",
    "        m = int(row[\"time_sec\"]) // 60\n",
    "        payload = {\"dropoff\": [], \"recovery\": [], \"pickup\": []}\n",
    "        ctrs = key_status_counter.get((term, m), {})\n",
    "\n",
    "        for st in (\"dropoff\", \"recovery\", \"pickup\"):\n",
    "            ctr = ctrs.get(st, Counter())\n",
    "            payload[st] = [key_to_dict(k, n) for k, n in ctr.items()]\n",
    "\n",
    "        json_list.append(json.dumps(payload, ensure_ascii=False))\n",
    "\n",
    "    out = per_terminal_df.copy()\n",
    "    out[\"description_json\"] = json_list\n",
    "    return out\n",
    "\n",
    "\n",
    "# =======================\n",
    "# Core computation (one service_id)\n",
    "# =======================\n",
    "def build_terminal_minute_counts_for_service(\n",
    "    gtfs_dir: str,\n",
    "    service_id: int,\n",
    "    candidate_xlsx: str = CANDIDATE_XLSX,\n",
    "    block_meta_csv: str = BLOCK_META_CSV,\n",
    "    deadhead_dist_m: int = DEADHEAD_DIST_M,\n",
    "):\n",
    "    # ---- Load candidate terminals (stop_code based) ----\n",
    "    cand = pd.read_excel(candidate_xlsx)\n",
    "\n",
    "    stopcode_to_terminal = {}\n",
    "    terminals = []\n",
    "\n",
    "    for _, r in cand.iterrows():\n",
    "        term = str(r[\"stop_name_simple\"])\n",
    "        terminals.append(term)\n",
    "        codes = r[\"stop_code\"]\n",
    "\n",
    "        if isinstance(codes, str):\n",
    "            try:\n",
    "                codes = eval(codes)\n",
    "            except Exception:\n",
    "                codes = [codes]\n",
    "        if not isinstance(codes, (list, tuple, np.ndarray)):\n",
    "            codes = [codes]\n",
    "\n",
    "        for c in codes:\n",
    "            sc = _norm_stop_code(c)\n",
    "            if sc:\n",
    "                stopcode_to_terminal[sc] = term\n",
    "\n",
    "    candidate_stop_codes = set(stopcode_to_terminal.keys())\n",
    "    candidate_terminal_set = set(terminals)\n",
    "\n",
    "    # ---- Load GTFS ----\n",
    "    stops = pd.read_csv(os.path.join(gtfs_dir, \"stops.txt\"), dtype={\"stop_id\": str, \"stop_code\": str})\n",
    "    trips = pd.read_csv(os.path.join(gtfs_dir, \"trips.txt\"), dtype={\"trip_id\": str, \"block_id\": str, \"route_id\": str})\n",
    "    stop_times = pd.read_csv(os.path.join(gtfs_dir, \"stop_times.txt\"), dtype={\"trip_id\": str, \"stop_id\": str})\n",
    "    routes = pd.read_csv(os.path.join(gtfs_dir, \"routes.txt\"), dtype={\"route_id\": str, \"route_short_name\": str})\n",
    "\n",
    "    if \"service_id\" not in trips.columns:\n",
    "        raise ValueError(\"trips.txt has no 'service_id' column. Please confirm your GTFS feed format.\")\n",
    "\n",
    "    trips[\"service_id\"] = pd.to_numeric(trips[\"service_id\"], errors=\"coerce\")\n",
    "    trips = trips[(trips[\"service_id\"] == service_id)].dropna(subset=[\"block_id\"]).copy()\n",
    "    if trips.empty:\n",
    "        return pd.DataFrame(), pd.DataFrame(), pd.DataFrame()\n",
    "\n",
    "    # normalize trips block_id\n",
    "    trips[\"block_id_norm\"] = trips[\"block_id\"].apply(norm_block_id)\n",
    "    trips[\"block_id\"] = trips[\"block_id_norm\"]\n",
    "    trips = trips.dropna(subset=[\"block_id\"]).copy()\n",
    "\n",
    "    # route short name\n",
    "    routeid_to_short = routes.set_index(\"route_id\")[\"route_short_name\"].to_dict()\n",
    "    trips[\"route_short_name\"] = trips[\"route_id\"].map(routeid_to_short)\n",
    "\n",
    "    # stops mapping: stop_id -> stop_code + lat/lon\n",
    "    if \"stop_code\" not in stops.columns:\n",
    "        raise ValueError(\"stops.txt has no stop_code column. Confirm your GTFS feed.\")\n",
    "\n",
    "    stops[\"stop_code_norm\"] = stops[\"stop_code\"].apply(_norm_stop_code)\n",
    "    stop_id_to_code = stops.set_index(\"stop_id\")[\"stop_code_norm\"].to_dict()\n",
    "    stop_id_to_lat = stops.set_index(\"stop_id\")[\"stop_lat\"].to_dict()\n",
    "    stop_id_to_lon = stops.set_index(\"stop_id\")[\"stop_lon\"].to_dict()\n",
    "\n",
    "    # ---- Load block meta table and normalize block_id ----\n",
    "    meta = pd.read_csv(block_meta_csv)\n",
    "    if \"service_id\" not in meta.columns:\n",
    "        raise ValueError(\"block meta csv must include service_id column.\")\n",
    "\n",
    "    meta[\"service_id\"] = pd.to_numeric(meta[\"service_id\"], errors=\"coerce\")\n",
    "    meta = meta[meta[\"service_id\"] == service_id].copy()\n",
    "\n",
    "    col_block_id = \"block_id\"\n",
    "    col_line_group = \"line_group\"\n",
    "    col_block_number = \"block_number\"\n",
    "    col_depot = \"depot_code\"\n",
    "\n",
    "    asset_col = \"asset_class\"\n",
    "\n",
    "    meta[\"block_id_norm\"] = meta[col_block_id].apply(norm_block_id)\n",
    "\n",
    "    meta_std = pd.DataFrame({\n",
    "        \"block_id\": meta[\"block_id_norm\"].astype(str),\n",
    "        \"line_group\": meta[col_line_group] if col_line_group else np.nan,\n",
    "        \"block_number\": meta[col_block_number] if col_block_number else np.nan,\n",
    "        \"depot_code\": meta[col_depot] if col_depot else np.nan,\n",
    "        \"asset_class\": meta[asset_col] if asset_col else np.nan,\n",
    "    }).drop_duplicates(\"block_id\")\n",
    "\n",
    "    # ---- Trip endpoints ----\n",
    "    stop_times[\"arrival_sec\"] = stop_times[\"arrival_time\"].apply(parse_gtfs_time_to_sec)\n",
    "    stop_times[\"departure_sec\"] = stop_times[\"departure_time\"].apply(parse_gtfs_time_to_sec)\n",
    "    stop_times[\"stop_sequence\"] = pd.to_numeric(stop_times[\"stop_sequence\"], errors=\"coerce\")\n",
    "\n",
    "    st_sorted = stop_times.sort_values([\"trip_id\", \"stop_sequence\"])\n",
    "\n",
    "    first = st_sorted.groupby(\"trip_id\", as_index=False).first()[[\"trip_id\", \"stop_id\", \"departure_sec\"]]\n",
    "    first = first.rename(columns={\"stop_id\": \"first_stop_id\", \"departure_sec\": \"trip_start_sec\"})\n",
    "\n",
    "    last = st_sorted.groupby(\"trip_id\", as_index=False).last()[[\"trip_id\", \"stop_id\", \"arrival_sec\"]]\n",
    "    last = last.rename(columns={\"stop_id\": \"last_stop_id\", \"arrival_sec\": \"trip_end_sec\"})\n",
    "\n",
    "    trip_ends = (\n",
    "        first.merge(last, on=\"trip_id\", how=\"inner\")\n",
    "             .merge(trips[[\"trip_id\", \"block_id\", \"route_short_name\"]], on=\"trip_id\", how=\"inner\")\n",
    "             .merge(meta_std, on=\"block_id\", how=\"left\")\n",
    "    )\n",
    "\n",
    "    trip_ends[\"first_stop_code\"] = trip_ends[\"first_stop_id\"].map(stop_id_to_code)\n",
    "    trip_ends[\"last_stop_code\"] = trip_ends[\"last_stop_id\"].map(stop_id_to_code)\n",
    "    trip_ends[\"first_lat\"] = trip_ends[\"first_stop_id\"].map(stop_id_to_lat)\n",
    "    trip_ends[\"first_lon\"] = trip_ends[\"first_stop_id\"].map(stop_id_to_lon)\n",
    "    trip_ends[\"last_lat\"] = trip_ends[\"last_stop_id\"].map(stop_id_to_lat)\n",
    "    trip_ends[\"last_lon\"] = trip_ends[\"last_stop_id\"].map(stop_id_to_lon)\n",
    "\n",
    "    trip_ends[\"first_in_candidates\"] = trip_ends[\"first_stop_code\"].isin(candidate_stop_codes)\n",
    "    trip_ends[\"last_in_candidates\"] = trip_ends[\"last_stop_code\"].isin(candidate_stop_codes)\n",
    "    trip_ends[\"first_terminal\"] = trip_ends[\"first_stop_code\"].map(stopcode_to_terminal)\n",
    "    trip_ends[\"last_terminal\"] = trip_ends[\"last_stop_code\"].map(stopcode_to_terminal)\n",
    "\n",
    "    trip_ends = trip_ends.sort_values([\"block_id\", \"trip_start_sec\"]).reset_index(drop=True)\n",
    "\n",
    "    # ---- Build intervals ----\n",
    "    intervals = []\n",
    "\n",
    "    def add_interval(row, terminal, status, s, e, prev_route, next_route):\n",
    "        if terminal not in candidate_terminal_set:\n",
    "            return\n",
    "        if e <= s:\n",
    "            return\n",
    "        intervals.append({\n",
    "            \"service_id\": service_id,\n",
    "            \"block_id\": str(row[\"block_id\"]),\n",
    "            \"terminal\": terminal,\n",
    "            \"status\": status,\n",
    "            \"start_sec\": int(s),\n",
    "            \"end_sec\": int(e),\n",
    "            \"line_group\": row.get(\"line_group\", np.nan),\n",
    "            \"block_number\": row.get(\"block_number\", np.nan),\n",
    "            \"asset_class\": row.get(\"asset_class\", np.nan),\n",
    "            \"depot_code\": row.get(\"depot_code\", np.nan),\n",
    "            \"prev_route_short_name\": prev_route,\n",
    "            \"next_route_short_name\": next_route,\n",
    "        })\n",
    "\n",
    "    for block_id, g in trip_ends.groupby(\"block_id\", sort=False):\n",
    "        g = g.sort_values(\"trip_start_sec\").reset_index(drop=True)\n",
    "\n",
    "        # Case 3: first trip arrival (if first stop in candidates)\n",
    "        if bool(g.loc[0, \"first_in_candidates\"]) and pd.notna(g.loc[0, \"trip_start_sec\"]):\n",
    "            dep = int(g.loc[0, \"trip_start_sec\"])\n",
    "            arr = max(0, dep - ARRIVE_BEFORE_DEP_SEC)\n",
    "\n",
    "            rec_end = max(arr, dep - PICK_SEC)\n",
    "            rec_start = max(arr, dep - PICK_SEC - RECOVERY_SEC)\n",
    "\n",
    "            add_interval(g.loc[0], g.loc[0, \"first_terminal\"], \"recovery\", rec_start, rec_end,\n",
    "                         prev_route=None, next_route=g.loc[0, \"route_short_name\"])\n",
    "            add_interval(g.loc[0], g.loc[0, \"first_terminal\"], \"pickup\", dep - PICK_SEC, dep,\n",
    "                         prev_route=None, next_route=g.loc[0, \"route_short_name\"])\n",
    "\n",
    "        # Pairwise logic\n",
    "        for i in range(len(g)):\n",
    "            cur = g.loc[i]\n",
    "            cur_end = cur[\"trip_end_sec\"]\n",
    "            cur_last_in = bool(cur[\"last_in_candidates\"])\n",
    "            cur_last_term = cur[\"last_terminal\"]\n",
    "            prev_route = cur[\"route_short_name\"]\n",
    "\n",
    "            # Case 4: last trip dropoff\n",
    "            if i == len(g) - 1:\n",
    "                if cur_last_in and pd.notna(cur_end):\n",
    "                    arr = int(cur_end)\n",
    "                    add_interval(cur, cur_last_term, \"dropoff\", arr, arr + DROP_SEC,\n",
    "                                 prev_route=prev_route, next_route=None)\n",
    "                break\n",
    "\n",
    "            nxt = g.loc[i + 1]\n",
    "            nxt_dep = nxt[\"trip_start_sec\"]\n",
    "            nxt_first_in = bool(nxt[\"first_in_candidates\"])\n",
    "            nxt_first_term = nxt[\"first_terminal\"]\n",
    "            next_route = nxt[\"route_short_name\"]\n",
    "\n",
    "            if pd.isna(cur_end) or pd.isna(nxt_dep):\n",
    "                continue\n",
    "\n",
    "            arr = int(cur_end)\n",
    "            dep = int(nxt_dep)\n",
    "\n",
    "            d_m = None\n",
    "            try:\n",
    "                d_m = haversine_m(cur[\"last_lat\"], cur[\"last_lon\"], nxt[\"first_lat\"], nxt[\"first_lon\"])\n",
    "            except Exception:\n",
    "                d_m = None\n",
    "\n",
    "            same_terminal_candidate = (\n",
    "                cur_last_in and nxt_first_in and\n",
    "                (cur_last_term == nxt_first_term) and\n",
    "                (d_m is not None and d_m <= deadhead_dist_m)\n",
    "            )\n",
    "            deadhead_like = (d_m is not None and d_m > deadhead_dist_m)\n",
    "\n",
    "            # Case 1: same terminal\n",
    "            if same_terminal_candidate:\n",
    "                for status, s, e in safe_split_same_terminal(arr, dep):\n",
    "                    add_interval(cur, cur_last_term, status, s, e,\n",
    "                                 prev_route=prev_route, next_route=next_route)\n",
    "                continue\n",
    "\n",
    "            # Case 2: interline/deadhead\n",
    "            if deadhead_like:\n",
    "                if cur_last_in and nxt_first_in:\n",
    "                    add_interval(cur, cur_last_term, \"dropoff\", arr, arr + DROP_SEC,\n",
    "                                 prev_route=prev_route, next_route=next_route)\n",
    "\n",
    "                    dest_dep = dep\n",
    "                    dest_arr = max(0, dest_dep - ARRIVE_BEFORE_DEP_SEC)\n",
    "                    rec_end = max(dest_arr, dest_dep - PICK_SEC)\n",
    "                    rec_start = max(dest_arr, rec_end - RECOVERY_SEC)\n",
    "\n",
    "                    add_interval(nxt, nxt_first_term, \"recovery\", rec_start, rec_end,\n",
    "                                 prev_route=prev_route, next_route=next_route)\n",
    "                    add_interval(nxt, nxt_first_term, \"pickup\", dest_dep - PICK_SEC, dest_dep,\n",
    "                                 prev_route=prev_route, next_route=next_route)\n",
    "\n",
    "                elif cur_last_in and (not nxt_first_in):\n",
    "                    add_interval(cur, cur_last_term, \"dropoff\", arr, arr + DROP_SEC,\n",
    "                                 prev_route=prev_route, next_route=next_route)\n",
    "\n",
    "                elif (not cur_last_in) and nxt_first_in:\n",
    "                    dest_dep = dep\n",
    "                    dest_arr = max(0, dest_dep - ARRIVE_BEFORE_DEP_SEC)\n",
    "                    rec_end = max(dest_arr, dest_dep - PICK_SEC)\n",
    "                    rec_start = max(dest_arr, rec_end - RECOVERY_SEC)\n",
    "\n",
    "                    add_interval(nxt, nxt_first_term, \"recovery\", rec_start, rec_end,\n",
    "                                 prev_route=prev_route, next_route=next_route)\n",
    "                    add_interval(nxt, nxt_first_term, \"pickup\", dest_dep - PICK_SEC, dest_dep,\n",
    "                                 prev_route=prev_route, next_route=next_route)\n",
    "\n",
    "                continue\n",
    "\n",
    "            # everything else ignored\n",
    "\n",
    "    intervals_df = pd.DataFrame(intervals)\n",
    "    if intervals_df.empty:\n",
    "        return intervals_df, pd.DataFrame(), pd.DataFrame()\n",
    "\n",
    "    # ---- System-wide minute table (keep your original diff approach; duplication here is fine because it's total-level)\n",
    "    max_end = int(intervals_df[\"end_sec\"].max())\n",
    "    nmin = (max_end // 60) + 3\n",
    "\n",
    "    def make_series(status: str):\n",
    "        diff = np.zeros(nmin, dtype=int)\n",
    "        sub = intervals_df[intervals_df[\"status\"] == status]\n",
    "        for s, e in zip(sub[\"start_sec\"].values, sub[\"end_sec\"].values):\n",
    "            add_interval_minute_diff(diff, int(s), int(e))\n",
    "        return np.cumsum(diff)[:nmin - 1]\n",
    "\n",
    "    dropoff = make_series(\"dropoff\")\n",
    "    recovery = make_series(\"recovery\")\n",
    "    pickup = make_series(\"pickup\")\n",
    "    total = dropoff + recovery + pickup\n",
    "\n",
    "    minutes = np.arange(len(total), dtype=int)\n",
    "    time_sec = (minutes * 60).astype(int)\n",
    "    day_offset = (time_sec // 86400).astype(int)\n",
    "    is_next_day = (day_offset >= 1).astype(int)\n",
    "\n",
    "    minute_df = pd.DataFrame({\n",
    "        \"service_id\": service_id,\n",
    "        \"service_label\": SERVICE_MAP.get(service_id, str(service_id)),\n",
    "        \"time_sec\": time_sec,\n",
    "        \"time_hhmm\": [sec_to_hhmm(int(s)) for s in time_sec],\n",
    "        \"day_offset\": day_offset,\n",
    "        \"is_next_day\": is_next_day,\n",
    "        \"total_buses_in_candidates\": total,\n",
    "        \"dropoff_buses\": dropoff,\n",
    "        \"recovery_buses\": recovery,\n",
    "        \"pickup_buses\": pickup,\n",
    "    })\n",
    "    minute_df = minute_df[minute_df[\"total_buses_in_candidates\"] > 0].reset_index(drop=True)\n",
    "\n",
    "    # ---- Per-terminal minute table with dominant-duration (fixes duplication)\n",
    "    assign_df = build_minute_assignment(intervals_df)\n",
    "    per_terminal_df = build_per_terminal_minute_from_assignment(\n",
    "        assign_df=assign_df,\n",
    "        service_id=service_id,\n",
    "        service_label=SERVICE_MAP.get(service_id, str(service_id)),\n",
    "    )\n",
    "\n",
    "    # ---- Build a per-minute lookup of metadata for JSON\n",
    "    # We need prev/next routes at the minute level (which boundary is active in that minute).\n",
    "    # Approach:\n",
    "    # - For each (terminal, minute, block_id), pick the interval row that overlaps that minute with MAX seconds\n",
    "    #   and inherit its metadata (line_group, block_number, asset_class, depot_code, prev_route, next_route).\n",
    "    interval_acc = defaultdict(int)\n",
    "    interval_meta = {}\n",
    "\n",
    "    for _, r in intervals_df.iterrows():\n",
    "        term = str(r[\"terminal\"])\n",
    "        block_id = str(r[\"block_id\"])\n",
    "        status = r[\"status\"]\n",
    "        s = int(r[\"start_sec\"])\n",
    "        e = int(r[\"end_sec\"])\n",
    "        if e <= s:\n",
    "            continue\n",
    "        m0 = s // 60\n",
    "        m1 = (e - 1) // 60\n",
    "        for m in range(m0, m1 + 1):\n",
    "            ms = m * 60\n",
    "            me = (m + 1) * 60\n",
    "            overlap = min(e, me) - max(s, ms)\n",
    "            if overlap <= 0:\n",
    "                continue\n",
    "            key = (term, m, block_id, status)\n",
    "            interval_acc[key] += overlap\n",
    "            # store metadata (same for the interval)\n",
    "            interval_meta[key] = {\n",
    "                \"line_group\": r.get(\"line_group\", np.nan),\n",
    "                \"block_number\": r.get(\"block_number\", np.nan),\n",
    "                \"asset_class\": r.get(\"asset_class\", np.nan),\n",
    "                \"depot_code\": r.get(\"depot_code\", np.nan),\n",
    "                \"prev_route_short_name\": r.get(\"prev_route_short_name\", None),\n",
    "                \"next_route_short_name\": r.get(\"next_route_short_name\", None),\n",
    "            }\n",
    "\n",
    "    # Now for each assigned (terminal, minute, block_id), find the best matching interval key for that status\n",
    "    bus_lookup_rows = []\n",
    "    if not assign_df.empty:\n",
    "        for _, arow in assign_df.iterrows():\n",
    "            term = str(arow[\"terminal\"])\n",
    "            m = int(arow[\"minute_index\"])\n",
    "            time_s = int(arow[\"time_sec\"])\n",
    "            block_id = str(arow[\"block_id\"])\n",
    "            status = arow[\"status\"]\n",
    "\n",
    "            # pick the interval key for this (term,m,block,status) with max overlap\n",
    "            best_k = None\n",
    "            best_sec = -1\n",
    "            k = (term, m, block_id, status)\n",
    "            sec = interval_acc.get(k, 0)\n",
    "            if sec > best_sec:\n",
    "                best_sec = sec\n",
    "                best_k = k\n",
    "\n",
    "            meta_payload = interval_meta.get(best_k, {})\n",
    "            bus_lookup_rows.append({\n",
    "                \"terminal\": term,\n",
    "                \"block_id\": block_id,\n",
    "                \"time_sec\": time_s,\n",
    "                **meta_payload\n",
    "            })\n",
    "\n",
    "    bus_lookup_df = pd.DataFrame(bus_lookup_rows)\n",
    "    # Add JSON (based on assignment, so no double-count possible)\n",
    "    per_terminal_df = add_description_json_to_per_terminal_from_assignment(\n",
    "        per_terminal_df=per_terminal_df,\n",
    "        assign_df=assign_df,\n",
    "        bus_lookup_df=bus_lookup_df,\n",
    "    )\n",
    "\n",
    "    # Sanity check: should always hold with dominant-duration\n",
    "    if not per_terminal_df.empty:\n",
    "        ok = (per_terminal_df[\"total\"] == (per_terminal_df[\"dropoff\"] + per_terminal_df[\"recovery\"] + per_terminal_df[\"pickup\"]))\n",
    "        if not bool(ok.all()):\n",
    "            bad_n = int((~ok).sum())\n",
    "            print(f\"[WARN] {bad_n} rows violate total == dropoff+recovery+pickup (should be 0).\")\n",
    "\n",
    "    return intervals_df, minute_df, per_terminal_df\n",
    "\n",
    "\n",
    "# =======================\n",
    "# Runner: save per service_id\n",
    "# =======================\n",
    "def run_services_and_save(gtfs_dir: str, out_dir: str):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    for sid in [1, 2, 3]:\n",
    "        intervals_df, minute_df, per_terminal_df = build_terminal_minute_counts_for_service(\n",
    "            gtfs_dir=gtfs_dir,\n",
    "            service_id=sid,\n",
    "            candidate_xlsx=CANDIDATE_XLSX,\n",
    "            block_meta_csv=BLOCK_META_CSV,\n",
    "            deadhead_dist_m=DEADHEAD_DIST_M,\n",
    "        )\n",
    "\n",
    "        label = SERVICE_MAP.get(sid, f\"service{sid}\")\n",
    "\n",
    "        intervals_path = os.path.join(out_dir, f\"terminal_intervals_{label}_service_id_{sid}.csv\")\n",
    "        minute_path = os.path.join(out_dir, f\"terminal_minute_counts_{label}_service_id_{sid}.csv\")\n",
    "        per_term_path = os.path.join(out_dir, f\"terminal_minute_counts_by_terminal_{label}_service_id_{sid}.csv\")\n",
    "\n",
    "        intervals_df.to_csv(intervals_path, index=False)\n",
    "        minute_df.to_csv(minute_path, index=False)\n",
    "        per_terminal_df.to_csv(per_term_path, index=False)\n",
    "\n",
    "        print(f\"[OK] service_id={sid} ({label})\")\n",
    "        print(f\"  minute:    {minute_path}\")\n",
    "        print(f\"  per_term:  {per_term_path}\")\n",
    "        print(f\"  intervals: {intervals_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4a2c4c2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] service_id=1 (MF)\n",
      "  minute:    output_term\\terminal_minute_counts_MF_service_id_1.csv\n",
      "  per_term:  output_term\\terminal_minute_counts_by_terminal_MF_service_id_1.csv\n",
      "  intervals: output_term\\terminal_intervals_MF_service_id_1.csv\n",
      "[OK] service_id=2 (Sat)\n",
      "  minute:    output_term\\terminal_minute_counts_Sat_service_id_2.csv\n",
      "  per_term:  output_term\\terminal_minute_counts_by_terminal_Sat_service_id_2.csv\n",
      "  intervals: output_term\\terminal_intervals_Sat_service_id_2.csv\n",
      "[OK] service_id=3 (Sun)\n",
      "  minute:    output_term\\terminal_minute_counts_Sun_service_id_3.csv\n",
      "  per_term:  output_term\\terminal_minute_counts_by_terminal_Sun_service_id_3.csv\n",
      "  intervals: output_term\\terminal_intervals_Sun_service_id_3.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    gtfs_dir = r\"gtfs_bus_only\"          \n",
    "    out_dir = r\"output_term\"    \n",
    "    run_services_and_save(gtfs_dir, out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0cab85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ac160a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c70bf3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622bb482",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1e06ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9013a40b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Building map for service_id=1 (MF) ===\n",
      "[OK] Map saved to output_term/terminal_stations_grouped_map_MF_service_id_1.html\n",
      "\n",
      "=== Building map for service_id=2 (Sat) ===\n",
      "[OK] Map saved to output_term/terminal_stations_grouped_map_Sat_service_id_2.html\n",
      "\n",
      "=== Building map for service_id=3 (Sun) ===\n",
      "[OK] Map saved to output_term/terminal_stations_grouped_map_Sun_service_id_3.html\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "import json\n",
    "import folium\n",
    "\n",
    "# =========================\n",
    "# Config\n",
    "# =========================\n",
    "GTFS_DIR = \"gtfs_bus_only\"\n",
    "\n",
    "TRIPS_PATH = os.path.join(GTFS_DIR, \"trips.txt\")\n",
    "STOP_TIMES_PATH = os.path.join(GTFS_DIR, \"stop_times.txt\")\n",
    "STOPS_PATH = os.path.join(GTFS_DIR, \"stops.txt\")\n",
    "ROUTES_PATH = os.path.join(GTFS_DIR, \"routes.txt\")\n",
    "\n",
    "BLOCK_SUMMARY_PATH = \"block_analysis_final.csv\"\n",
    "\n",
    "SERVICE_MAP = {1: \"MF\", 2: \"Sat\", 3: \"Sun\"}\n",
    "\n",
    "# Candidate highlights (optional)\n",
    "CANDIDATE_TXT = \"on_route_charger_location.txt\"  # if you still want green highlights\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Helpers\n",
    "# =========================\n",
    "def norm_block_id(x):\n",
    "    \"\"\"Normalize block_id to comparable string: handles int, float-like '21003.0', whitespace.\"\"\"\n",
    "    if pd.isna(x):\n",
    "        return None\n",
    "    s = str(x).strip()\n",
    "    if re.fullmatch(r\"\\d+\\.0\", s):\n",
    "        return s[:-2]\n",
    "    try:\n",
    "        f = float(s)\n",
    "        if f.is_integer():\n",
    "            return str(int(f))\n",
    "    except Exception:\n",
    "        pass\n",
    "    return s\n",
    "\n",
    "\n",
    "def classify_bus_len(asset_class: str) -> str:\n",
    "    \"\"\"\n",
    "    Map asset_class into:\n",
    "      28ft, 40ft, 60ft, 44ft_dd (double decker), trolley, other\n",
    "    \"\"\"\n",
    "    if not isinstance(asset_class, str):\n",
    "        return \"other\"\n",
    "\n",
    "    s = asset_class.strip().lower()\n",
    "\n",
    "    # --- Trolley buses: starts with EL, but NOT ending with QC ---\n",
    "    # e.g., EL40LF -> trolley\n",
    "    #       EL40LFQC -> NOT trolley\n",
    "    if s.startswith(\"el\") and (not s.endswith(\"qc\")):\n",
    "        return \"trolley\"\n",
    "\n",
    "    # --- 60-foot articulated ---\n",
    "    if \"60\" in s or \"60ft\" in s or \"artic\" in s or \"articulated\" in s:\n",
    "        return \"60ft\"\n",
    "\n",
    "    # --- 44-foot double decker ---\n",
    "    if \"44\" in s or \"double\" in s or \"dd\" in s or \"decker\" in s:\n",
    "        return \"44ft_dd\"\n",
    "\n",
    "    # --- 40-foot standard ---\n",
    "    if \"40\" in s or \"40ft\" in s:\n",
    "        return \"40ft\"\n",
    "\n",
    "    # --- 28-foot ---\n",
    "    if \"28\" in s or \"28ft\" in s:\n",
    "        return \"28ft\"\n",
    "\n",
    "    return \"other\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def union_sets(series):\n",
    "    combined = set()\n",
    "    for x in series:\n",
    "        if pd.isna(x):\n",
    "            continue\n",
    "        if isinstance(x, (set, list, tuple, np.ndarray)):\n",
    "            combined |= set(x)\n",
    "        else:\n",
    "            combined.add(x)\n",
    "    return combined\n",
    "\n",
    "\n",
    "def safe_list(x):\n",
    "    if isinstance(x, (set, list, tuple, np.ndarray)):\n",
    "        return list(x)\n",
    "    return [x]\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Load base files once\n",
    "# =========================\n",
    "trips_all = pd.read_csv(TRIPS_PATH, dtype={\"trip_id\": str, \"route_id\": str, \"block_id\": str})\n",
    "stop_times_all = pd.read_csv(STOP_TIMES_PATH, dtype={\"trip_id\": str, \"stop_id\": str})\n",
    "stops = pd.read_csv(STOPS_PATH, dtype={\"stop_id\": str, \"stop_code\": str})\n",
    "routes = pd.read_csv(ROUTES_PATH, dtype={\"route_id\": str, \"route_short_name\": str})\n",
    "\n",
    "block_success_summary_all = pd.read_csv(BLOCK_SUMMARY_PATH)\n",
    "\n",
    "# Normalize block_id types everywhere (important)\n",
    "trips_all[\"block_id\"] = trips_all[\"block_id\"].apply(norm_block_id)\n",
    "block_success_summary_all[\"block_id\"] = block_success_summary_all[\"block_id\"].apply(norm_block_id)\n",
    "\n",
    "# Ensure service_id exists in both sources\n",
    "if \"service_id\" not in trips_all.columns:\n",
    "    raise ValueError(\"trips.txt must contain 'service_id' column for MF/Sat/Sun split.\")\n",
    "if \"service_id\" not in block_success_summary_all.columns:\n",
    "    raise ValueError(\"block_analysis_final.csv must contain 'service_id' column for MF/Sat/Sun split.\")\n",
    "\n",
    "trips_all[\"service_id\"] = pd.to_numeric(trips_all[\"service_id\"], errors=\"coerce\")\n",
    "block_success_summary_all[\"service_id\"] = pd.to_numeric(block_success_summary_all[\"service_id\"], errors=\"coerce\")\n",
    "\n",
    "# Stops: keep stop_code and lat/lon\n",
    "stops_key = (\n",
    "    stops.drop_duplicates(subset=[\"stop_code\"])\n",
    "         [[\"stop_code\", \"stop_name\", \"stop_lat\", \"stop_lon\", \"stop_id\"]]\n",
    "         .copy()\n",
    ")\n",
    "\n",
    "# Merge stop_code onto stop_times\n",
    "stop_times_all = stop_times_all.merge(\n",
    "    stops[[\"stop_id\", \"stop_code\"]],\n",
    "    on=\"stop_id\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Optional highlight set\n",
    "highlight_set = set()\n",
    "if os.path.exists(CANDIDATE_TXT):\n",
    "    curr_candidate = pd.read_csv(CANDIDATE_TXT)\n",
    "    curr_candidate[\"Location_new\"] = curr_candidate[\"Location\"].replace({\n",
    "        \"Commercial-Broadway Station\": \"N Grandview Hwy\",\n",
    "        \"Knight Street-Marine Drive\": \"Northbound Knight St Bridge Offramp\"\n",
    "    })\n",
    "    highlight_set = set(curr_candidate[\"Location_new\"].astype(str))\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Main loop: MF / Sat / Sun\n",
    "# =========================\n",
    "for sid, label in SERVICE_MAP.items():\n",
    "    print(f\"\\n=== Building map for service_id={sid} ({label}) ===\")\n",
    "\n",
    "    # ---- Filter block summary + trips by service_id\n",
    "    block_success_summary = block_success_summary_all[block_success_summary_all[\"service_id\"] == sid].copy()\n",
    "    valid_block_ids = set(block_success_summary[\"block_id\"].dropna().unique())\n",
    "\n",
    "    trips = trips_all[(trips_all[\"service_id\"] == sid) & (trips_all[\"block_id\"].isin(valid_block_ids))].copy()\n",
    "    if trips.empty:\n",
    "        print(f\"[WARN] No trips found for service_id={sid}. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    valid_trip_ids = set(trips[\"trip_id\"].unique())\n",
    "    stop_times = stop_times_all[stop_times_all[\"trip_id\"].isin(valid_trip_ids)].copy()\n",
    "\n",
    "    # ---- Add route_short_name\n",
    "    trips_with_route = trips.merge(\n",
    "        routes[[\"route_id\", \"route_short_name\"]],\n",
    "        on=\"route_id\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    trip_to_block = trips_with_route.set_index(\"trip_id\")[\"block_id\"].to_dict()\n",
    "    trip_to_route_short = trips_with_route.set_index(\"trip_id\")[\"route_short_name\"].to_dict()\n",
    "\n",
    "    # ---- block_id -> asset_class / depot (from your block_analysis_final.csv)\n",
    "    # Prefer asset_class_new if present\n",
    "    asset_col = \"asset_class_new\" if \"asset_class_new\" in block_success_summary.columns else \"asset_class\"\n",
    "    block_to_asset_class = block_success_summary.set_index(\"block_id\")[asset_col].to_dict()\n",
    "    block_to_depot = block_success_summary.set_index(\"block_id\")[\"depot_code\"].to_dict()\n",
    "\n",
    "    # ---- First/last stops by trip (by stop_code)\n",
    "    stop_times_sorted = stop_times.sort_values([\"trip_id\", \"stop_sequence\"])\n",
    "    first_stops = stop_times_sorted.groupby(\"trip_id\", as_index=False).first()[[\"trip_id\", \"stop_code\"]]\n",
    "    last_stops  = stop_times_sorted.groupby(\"trip_id\", as_index=False).last()[[\"trip_id\", \"stop_code\"]]\n",
    "\n",
    "    start_dict = defaultdict(set)\n",
    "    for _, row in first_stops.dropna(subset=[\"stop_code\"]).iterrows():\n",
    "        start_dict[str(row[\"stop_code\"])].add(row[\"trip_id\"])\n",
    "\n",
    "    end_dict = defaultdict(set)\n",
    "    for _, row in last_stops.dropna(subset=[\"stop_code\"]).iterrows():\n",
    "        end_dict[str(row[\"stop_code\"])].add(row[\"trip_id\"])\n",
    "\n",
    "    all_stop_codes = sorted(set(start_dict.keys()) | set(end_dict.keys()))\n",
    "\n",
    "    # ---- Base per-stop_code table\n",
    "    rows = []\n",
    "    for code in all_stop_codes:\n",
    "        start_set = start_dict.get(code, set())\n",
    "        end_set = end_dict.get(code, set())\n",
    "        all_trips = start_set | end_set\n",
    "        rows.append({\n",
    "            \"stop_code\": code,\n",
    "            \"start_trip_ids\": start_set,\n",
    "            \"end_trip_ids\": end_set,\n",
    "            \"all_trip_ids\": all_trips,\n",
    "            \"n_trips\": len(all_trips),\n",
    "        })\n",
    "\n",
    "    trip_sets_df = pd.DataFrame(rows)\n",
    "\n",
    "    # Merge stop metadata (lat/lon/name) by stop_code\n",
    "    result_df = trip_sets_df.merge(\n",
    "        stops_key.drop_duplicates(subset=[\"stop_code\"])[[\"stop_code\", \"stop_name\", \"stop_lat\", \"stop_lon\"]],\n",
    "        on=\"stop_code\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    # ---- Converters\n",
    "    def trips_to_block_set(trip_ids):\n",
    "        blocks = set()\n",
    "        for t in safe_list(trip_ids):\n",
    "            b = trip_to_block.get(t)\n",
    "            if b is not None and str(b).strip() != \"\":\n",
    "                blocks.add(str(b))\n",
    "        return blocks\n",
    "\n",
    "    def trips_to_route_short_set(trip_ids):\n",
    "        routes_short = set()\n",
    "        for t in safe_list(trip_ids):\n",
    "            r = trip_to_route_short.get(t)\n",
    "            if isinstance(r, str) and r.strip() != \"\":\n",
    "                routes_short.add(r)\n",
    "        return routes_short\n",
    "\n",
    "    def blocks_to_bus_type_counter(block_ids):\n",
    "        \"\"\"\n",
    "        Returns dict counting UNIQUE blocks by bus type.\n",
    "        Supported keys:\n",
    "        28ft, 40ft, 60ft, 44ft_dd, other\n",
    "        \"\"\"\n",
    "        ctr = Counter()\n",
    "\n",
    "        for b in safe_list(block_ids):\n",
    "            ac = block_to_asset_class.get(b)\n",
    "            ctr[classify_bus_len(ac)] += 1\n",
    "\n",
    "        # Ensure all expected keys exist (even if zero)\n",
    "        return {\n",
    "            \"28ft\": int(ctr.get(\"28ft\", 0)),\n",
    "            \"40ft (trolley excluded)\": int(ctr.get(\"40ft\", 0)),\n",
    "            \"60ft\": int(ctr.get(\"60ft\", 0)),\n",
    "            \"44ft_dd\": int(ctr.get(\"44ft_dd\", 0)),\n",
    "            \"40-foot trolley\": int(ctr.get(\"trolley\", 0)),\n",
    "            \"other\": int(ctr.get(\"other\", 0)),\n",
    "        }\n",
    "\n",
    "\n",
    "    def blocks_to_depot_counter(block_ids):\n",
    "        \"\"\"\n",
    "        Returns dict: { depot_code: n_blocks } counting UNIQUE blocks.\n",
    "        \"\"\"\n",
    "        ctr = Counter()\n",
    "        for b in safe_list(block_ids):\n",
    "            d = block_to_depot.get(b)\n",
    "            if isinstance(d, str) and d.strip() != \"\":\n",
    "                ctr[d.strip()] += 1\n",
    "            else:\n",
    "                ctr[\"UNKNOWN\"] += 1\n",
    "        return {k: int(v) for k, v in sorted(ctr.items(), key=lambda x: (-x[1], x[0]))}\n",
    "\n",
    "    # ---- Normalize stop_name_simple (same logic as you)\n",
    "    result_df[\"stop_name_simple\"] = (\n",
    "        result_df[\"stop_name\"]\n",
    "        .astype(str)\n",
    "        .str.replace(r\"\\s*@.*$\", \"\", regex=True)\n",
    "        .str.strip()\n",
    "    )\n",
    "\n",
    "    # ---- Group by stop_name_simple (terminal cluster)\n",
    "    grouped_df = (\n",
    "        result_df\n",
    "        .groupby(\"stop_name_simple\", as_index=False)\n",
    "        .agg({\n",
    "            \"stop_code\":      union_sets,\n",
    "            \"start_trip_ids\": union_sets,\n",
    "            \"end_trip_ids\":   union_sets,\n",
    "            \"all_trip_ids\":   union_sets,\n",
    "            \"stop_name\":      \"first\",\n",
    "            \"stop_lat\":       \"first\",\n",
    "            \"stop_lon\":       \"first\",\n",
    "        })\n",
    "    )\n",
    "\n",
    "    grouped_df[\"num_trip_total\"]  = grouped_df[\"all_trip_ids\"].apply(len)\n",
    "    grouped_df[\"num_trip_starts\"] = grouped_df[\"start_trip_ids\"].apply(len)\n",
    "    grouped_df[\"num_trip_ends\"]   = grouped_df[\"end_trip_ids\"].apply(len)\n",
    "\n",
    "    grouped_df[\"stop_code\"]      = grouped_df[\"stop_code\"].apply(lambda s: sorted(map(str, s)))\n",
    "    grouped_df[\"start_trip_ids\"] = grouped_df[\"start_trip_ids\"].apply(lambda s: sorted(map(str, s)))\n",
    "    grouped_df[\"end_trip_ids\"]   = grouped_df[\"end_trip_ids\"].apply(lambda s: sorted(map(str, s)))\n",
    "    grouped_df[\"all_trip_ids\"]   = grouped_df[\"all_trip_ids\"].apply(lambda s: sorted(map(str, s)))\n",
    "\n",
    "    grouped_df[\"block_ids\"] = grouped_df[\"all_trip_ids\"].apply(trips_to_block_set)\n",
    "    grouped_df[\"num_unique_blocks\"] = grouped_df[\"block_ids\"].apply(lambda s: len(set(s)))\n",
    "    grouped_df[\"route_short_names\"] = grouped_df[\"all_trip_ids\"].apply(trips_to_route_short_set)\n",
    "\n",
    "    # NEW popup dicts requested:\n",
    "    grouped_df[\"unique_blocks_by_bus_type\"] = grouped_df[\"block_ids\"].apply(blocks_to_bus_type_counter)\n",
    "    grouped_df[\"unique_blocks_by_depot\"] = grouped_df[\"block_ids\"].apply(blocks_to_depot_counter)\n",
    "\n",
    "    # Make lists for display\n",
    "    grouped_df[\"block_ids\"] = grouped_df[\"block_ids\"].apply(lambda s: sorted(list(set(s))))\n",
    "    grouped_df[\"route_short_names\"] = grouped_df[\"route_short_names\"].apply(lambda s: sorted(list(set(s))))\n",
    "\n",
    "    # ---- Build folium map\n",
    "    center_lat = grouped_df[\"stop_lat\"].mean()\n",
    "    center_lon = grouped_df[\"stop_lon\"].mean()\n",
    "\n",
    "    m = folium.Map(\n",
    "        location=[center_lat, center_lon],\n",
    "        zoom_start=11,\n",
    "        tiles=\"CartoDB positron\"\n",
    "    )\n",
    "\n",
    "    max_activity = grouped_df[\"num_trip_total\"].max() if len(grouped_df) else 0\n",
    "\n",
    "    for _, row in grouped_df.iterrows():\n",
    "        activity = row[\"num_trip_total\"]\n",
    "        radius = 5 + 25 * (activity / max_activity) if max_activity > 0 else 5\n",
    "\n",
    "        stop_codes_str = \", \".join(sorted(map(str, row[\"stop_code\"])))\n",
    "\n",
    "        route_short_names = row.get(\"route_short_names\", [])\n",
    "        routes_str = \", \".join(map(str, route_short_names)) if isinstance(route_short_names, (list, set, tuple)) else str(route_short_names)\n",
    "\n",
    "        # NEW dict fields\n",
    "        bus_type_dict = row.get(\"unique_blocks_by_bus_type\", {})\n",
    "        depot_dict = row.get(\"unique_blocks_by_depot\", {})\n",
    "\n",
    "        # Marker color: highlight terminals if in candidate list\n",
    "        if str(row[\"stop_name_simple\"]) in highlight_set:\n",
    "            color = fill_color = \"green\"\n",
    "        else:\n",
    "            color = fill_color = \"blue\"\n",
    "\n",
    "        folium.CircleMarker(\n",
    "            location=[row[\"stop_lat\"], row[\"stop_lon\"]],\n",
    "            radius=radius,\n",
    "            color=color,\n",
    "            fill=True,\n",
    "            fill_color=fill_color,\n",
    "            fill_opacity=0.4,\n",
    "            weight=1,\n",
    "            popup=folium.Popup(\n",
    "                f\"<b>{row['stop_name_simple']}</b><br>\"\n",
    "                f\"Service: {label} (service_id={sid})<br>\"\n",
    "                f\"Unique Blocks: {row['num_unique_blocks']}<br>\"\n",
    "                f\"Unique Routes: {routes_str}<br>\"\n",
    "                f\"Unique Blocks by Bus Type: {json.dumps(bus_type_dict, ensure_ascii=False)}<br>\"\n",
    "                f\"Unique Blocks by Depot: {json.dumps(depot_dict, ensure_ascii=False)}<br>\"\n",
    "                f\"Total Trips: {row['num_trip_total']}<br>\"\n",
    "                # f\"Trip Starts: {row['num_trip_starts']}<br>\"\n",
    "                # f\"Trip Ends: {row['num_trip_ends']}<br>\"\n",
    "                f\"Stop Codes: {stop_codes_str}<br>\",\n",
    "                max_width=420\n",
    "            )\n",
    "        ).add_to(m)\n",
    "\n",
    "    out_html = f\"output_term/terminal_stations_grouped_map_{label}_service_id_{sid}.html\"\n",
    "    m.save(out_html)\n",
    "    print(f\"[OK] Map saved to {out_html}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7569da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc76dd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c028d5f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8cceb9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gtfs-geo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
